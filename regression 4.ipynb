{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98169e0-524b-44b6-91d0-b4a55327648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a39747-47f7-41fa-900b-6985d6ebcd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "it is a linear regression technique that incorporates an L1 regularization term in the model training\n",
    "process. The key characteristic of Lasso Regression is its ability to perform feature selection and \n",
    "regularization simultaneously.\n",
    "Lasso Regression adds an L1 penalty term to the cost function, which helps in shrinking the \n",
    "coefficients of less important features to zero. This property leads to sparsity in the model, \n",
    "making it useful for feature selection\n",
    "Lasso Regression can automatically select relevant features by setting the coefficients of irrelevant\n",
    "features to zero. This feature selection capability can be advantageous when dealing with \n",
    "high-dimensional datasets.\n",
    "Lasso Regression helps in addressing the bias-variance tradeoff by penalizing the model for \n",
    "complexity. It can prevent overfitting by reducing the models complexity through the regularization \n",
    "term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816f735-188d-4de8-b7c5-aecb38337278",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710b722-f9cd-4213-8f03-e11779561aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "The main advantage of using Lasso Regression in feature selection is its ability to automatically\n",
    "perform variable selection by setting the coefficients of irrelevant features to zero. This\n",
    "property leads to sparse models where only the most relevant features are retained, simplifying the \n",
    "model and enhancing interpretability.\n",
    "Lasso Regression helps in reducing overfitting by effectively shrinking the less important feature \n",
    "coefficients, which can improve the models generalization performance on unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e68206-f4d6-4fe1-8817-2318ce5c8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ef5f6-4af1-4353-ae84-9a63bdc3ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Lasso Regression, the coefficients can be interpreted based on their magnitude, with non-zero \n",
    "coefficients indicating feature importance.\n",
    "The regularization process drives some coefficients to zero, leading to automatic feature \n",
    "selection and simpler, more interpretable models.\n",
    "the sign of the coefficient shows the direction of the relationship, while the regularization \n",
    "strength influences sparsity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69479d12-fb98-4ad4-b106-7371ba565748",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect\n",
    "the model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3333ea-b837-49fa-9db0-2a213c95d866",
   "metadata": {},
   "outputs": [],
   "source": [
    "In Lasso Regression, the tuning parameter Î± controls the regularization strength,\n",
    "influencing the sparsity of the model by shrinking coefficients towards zero. A higher alpha leads\n",
    "to more coefficients being forced to zero, facilitating feature selection and reducing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4ea7b-c08e-41e3-8f2b-4c8522b1c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9140e-ad7c-47b7-aa78-7c902a792a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for non-linear regression problems, Lasso Regression can be used in combination with polynomial \n",
    "features or other non-linear transformations. By expanding the feature space with non-linear terms, \n",
    "such as polynomial features or interactions, Lasso Regression can capture non-linear \n",
    "relationships and patterns in the data. This approach allows Lasso Regression to handle \n",
    "non-linearities and make it applicable to a broader range of regression problems beyond linear \n",
    "relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d97899-4fa9-40e3-8aef-09da43648ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88889388-5a29-4f8f-9eb6-737800553250",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge Regression and Lasso Regression differ primarily in the type of regularization they apply and\n",
    "their impact on feature selection. \n",
    "Ridge Regression utilizes L2 regularization, which adds the squared magnitude of coefficients to the\n",
    "cost function, effectively shrinking but not eliminating irrelevant features. \n",
    "while, Lasso Regression applies L1 regularization, encouraging sparsity by setting some coefficients \n",
    "to zero, thus performing automatic feature selection. \n",
    "This key difference makes Lasso Regression particularly useful for high-dimensional datasets and \n",
    "situations requiring sparse models. \n",
    "While Ridge Regression is suitable for reducing multicollinearity and improving model \n",
    "generalization, Lasso Regression stands out for its inherent feature selection capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464ebe7a-40f8-4be2-b557-210349ffe2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe318651-9b5e-4745-99c6-04de91902c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Lasso Regression can help address multicollinearity in input features to some extent. While \n",
    "multicollinearity can pose challenges for traditional regression models, Lasso Regression, with its \n",
    "L1 regularization, encourages sparsity by driving some coefficients to zero, effectively selecting a \n",
    "subset of features. \n",
    "In the context of multicollinearity, Lasso Regression tends to choose one feature from a group of \n",
    "highly correlated features and set the coefficients of the rest to zero. This feature selection \n",
    "property aids in mitigating the impact of multicollinearity by simplifying the model and emphasizing \n",
    "the most relevant features while disregarding redundant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42219f52-ee56-47dc-b042-6f4bf36fd033",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66364e1d-aa14-4f0f-89c5-4c53ec7cf91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cross-Validation: Utilize techniques such as k-fold cross-validation to evaluate the model's\n",
    "performance across different values of lambda. The value that yields the best performance, as \n",
    "measured by metrics like mean squared error or R-squared, can be considered optimal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
